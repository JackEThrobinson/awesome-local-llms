# Awesome Local LLMs

There are many ways of hosting LLMs locally for inference: from the command line (CLI) tools to full GUI desktop applications. This repo aims to curate all possible options for running local LLMs. These projects can overlap in scope and may split into different components of inference backend server and UI. GitHub repository metrics have been collected as a proxy for popularity and active maintenance.

For full table with all metrics go to this [Google Sheet](https://docs.google.com/spreadsheets/d/1Xv38p90V3GiJXjq0a3qc24056Vicn1I5MG6QiFE6nVE/edit?usp=sharing).
